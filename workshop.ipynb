{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Data exploration\n\n## Python\n\nPython stands out as a high-level programming language. Beyond its versatility and ease of learning, Python boasts an extensive ecosystem, enabling the utilization of a myriad of diverse libraries.\n\n## Modules, Packages and Libraries\n\rA module serves as a repository of functions or methods. A package, in turn, comprises a set of modules. A library is an assembly of packages. Libraries facilitate code reusability by offering pre-built features for diverse problems, thereby obviating the necessity to repeatedly recreate identical code.\nTo install a new library, usually, you can simply type in the terminal/command line: pip install [module_name].\n\nIn this workshop, we will make use of the following libraries:\n\n* numpy\n* pandas\n* scikit-learn\n* tensorflow\n* matplotlib\n* seaborn\n* plotl\n\nBelow are exemplified two ways of importing a library.y.st",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom numpy import array",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "In the first case, the library is imported in its entirety. In the second case, only a specific method is imported.\n\nImport all the remaining libraries in their entirety.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Dataset\n\nA dataset is a compilation of data, encompassing not only tabular forms but also various types such as images, text, and more. A tabular dataset is composed of objects and features, a set of characteristics/variables of a specific object. Variables enable the storage and representation of values. In Python, there is no need to predefine the variable type.\n\nType of variables:\n- string;\n- numeric (integer or float);\n- boolean;\n- lists;\n- tuples;\n- dictionaries.\n\n**Boolean**\n\nThe boolean objects are objects that are either true or false (1 or 0, respectively). These objects allow the evaluation of conditions. To evaluate conditions, operators such as **if**, **elif**, **else**, **and** and **or** are used. In other words, the if, elif, and else statements allow the evaluation of conditional premises and, eventually, take action based on them. The and and or operators allow the evaluation of joint premises.\n\nThe code below illustrates a if condition.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "k = 1\nif k == 1:\n    print(x)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Lists and dictionaries**\n\nLists are a way of storing objects and can be created using [] or through the list() notation. The dictionary can be created using {} or the dict() notation. Unlike lists, dictionaries are not inherently ordered; however, they operate on a key-value logic that allows storing data relationally.\n\nExamples of functions used in lists:\n* append()\n* del()\n* len()\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "list_numbers = [0,1,2,3,4]\nlist_numbers.append(\"A\")\nprint(list_numbers)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Remove the value 0.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "It is possible to add the previous list to a dictionary.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "dict_mouth = {\"jan\": 31, \"fev\": [27,28], \"mar\":31}\ndict_mouth[\"new_key\"] = list_numbers\nprint(dict_mouth)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Iterators**\n\nIterators enable traversing objects and object storages, evaluating values, and performing operations.\n* for\n* while\n\nThe code below illustrates an iteration of a list.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for number in list_numbers:\n    print(number)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## pandas\n\nAccording to its creators, \"pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool\".\n\n\nTypes of pandas data structures:\n- Series;\n- Dataframes.\n\nThe function read_csv() is used to read the file containing the data, thus generating a DataFrame.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "opened_data = pd.read_csv(\"data/all_stocks_5yr.csv\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The function head() allows visualization of the first rows of the table.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "opened_data.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "In turn, the info() function allows obtaining global information about the DataFrame.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "opened_data.info()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The describe() function also allows obtaining information about the table.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "opened_data.descrive()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "At times, not all columns have values (missing values). The dropna() function resolves this issue.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "opened_data.dropna(inplace=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "There are several tools that allow for category-based analysis.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "count_by_company = opened_data.groupby(\"Name\").count()\nprint(count_by_company)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The .loc attribute can be used to locate rows based on the label. It is also possible to combine the .loc attribute with boolean logic, thus making sub-partitions of the data according to conditions.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "opened_data.loc[opened_data[\"Name\"] == \"APP\"]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The iloc attribute allows for positional-based locating.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "opened_data.iloc[0:15,1]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#**Exercise: How many different years appear in the dataset?**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#**Exercise: What is the percentage of rows from 2018?**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#**Exercise: what is the percentage of rows for each year?**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Data visualization\n\n## seaborn\n\nSeaborn is a python library based on matplotlib that allows the creation of graphics. To customize the plot, it is necessary to delve deeper into the documentation of the library. \nFunctions for the production of graphics:\n* barplot()\n* boxplot()\n* violinplot()\n* lineplot()\n* heatmap()\n* histplot\n\nNow we will work with the file processed_stock_data.csv, a file that only contains the average closing values of the stock market per year. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "reformed_df = pd.read_csv(\"data/processed_stock_data.csv\")\nreformed_df.head(5)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "You can calculate the mean, standard deviation, and variance along an axis. Choosing 0 will calculate along the columns, while 1 will calculate along the rows.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "yearly_average_df = reformed_df.mean(axis = 0)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "The code below allows you to create a bar chart.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "sns.barplot(yearly_average_df, legend=False)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "It is possible to customize the chart according to individual preferences and the representation's objective by adding and modifying arguments.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.barplot(yearly_average_df, legend=False)\nplt.title(\"Yearly stock value\")\nplt.xticks(rotation=45)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Boxplots enable the graphical visualization of outliers.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.violinplot(reformed_df)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#**Exercise: Build a violinplot to further assess the distribution**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "#**Exercise: Identify the top 5 stocks with lowest closing in 2018 and build a lineplot with the years as the x-axis**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Data preprocessing\n\n## Outlier detection and handling\n\nAn outlier is a data point in the dataset that deviates significantly from the rest of the data or observations. If neglected, this type of data can disrupt the intended analysis. Outliers can be detected through the use of quartiles.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "quantile_low = reformed_df[\"2018_close\"].quantile(0.01)\nquantile_high  = reformed_df[\"2018_close\"].quantile(0.99)\n\nreformed_df_filtered = reformed_df[(reformed_df[\"2018_close\"] < quantile_high) & (reformed_df[\"2018_close\"] > quantile_low)]\n\nfig, axes = plt.subplots(1, 2)\nsns.violinplot(reformed_df, ax=axes[0])\nsns.violinplot(reformed_df_filtered, ax=axes[1])\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## scikit-learn\n\nThe scikit-learn contains a vast number of functions for machine learning, as well as data preprocessing functions. These functions enable the analysis and preparation of data to undergo machine learning processes.\n\n## Features, target variable\n\nBefore executing an ML protocol, it is necessary to split the data into features and target variable. The target variable is what we aim to predict, serving as the dependent variable. Features, on the other hand, are the independent variables used in predicting the target variable.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "features = reformed_df.drop(columns = [\"2018_close\"])\ntarget_variable = reformed_df[\"2018_close\"]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Data spliting\n\nNext, it is essential to split the dataset into training and testing sets, with the target variable isolated since it is the variable we want to predict. To do this, we will use the train_test_split function from sickit-learn.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\ntrain_features, test_features, train_target, test_target = train_test_split(features, target_variable, random_state = 42)\nprint(train_features.shape, test_features.shape, train_target.shape, test_target.shape)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Data Standardization/normalization\n\nData standardization aids in preparing datasets for analysis by normalizing features to a consistent scale. This process ensures that different features contribute equally to the analysis.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "scaler = StandardScaler()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "scaler.fit(train_features)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "scaled_train_features = scaler.transform(train_features)\nscaled_test_features = scaler.transform(test_features)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}